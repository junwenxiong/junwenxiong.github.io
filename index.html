<!DOCTYPE HTML>
<html lang="en">

<head>
	<meta name="google-site-verification" content="ufZa3XjmGskngtrL5yh3aMO6n4-QlFMFAyP2kxPsmRw" />
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<title>Junwen Xiong</title>

	<meta name="author" content="Junwen Xiong">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-BQ6J8EFY6Z"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'G-BQ6J8EFY6Z');
	</script>
</head>

<body>
	<table
		style="width:110%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
		<tbody>
			<tr style="padding:0px">
				<td style="padding:0px">
					<table
						style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr style="padding:0px">
								<td
									style="padding:2.5%;width:63%;vertical-align:middle">
									<p style="text-align:center">
										<name>Junwen Xiong </name>
									</p>
									<br>
									<p>
										Hi, I am Junwen Xiong (熊俊文), 
										a first-year Ph.D. student in the Department of Computer Science
										at Northwestern Polytechnical University, advised by <a href="https://teacher.nwpu.edu.cn/2011010119.html">Prof.
											Peng Zhang</a>.
									</p>
									<p>
										I'm broadly interested in multimodal
										learning (images, audio, video, etc.).
										My recent research lies in audio-visual
										speech separation, sound source
										localization.
									</p>

									<p style="text-align:center">
										<a href="data/email.txt">Email</a>
										&nbsp|&nbsp
										<a
											href="https://scholar.google.com/citations?user=7iM62mUAAAAJ&hl=zh-CN">Google
											Scholar</a> &nbsp|&nbsp
										<a
											href="https://github.com/junwenxiong">Github</a>
									</p>
								</td>
								<td style="width:50%;max-width:50%">
									<div>
										<a href="images/profile.jpg"><img
												style="width:100%;max-width:100%"
												alt="profile photo"
												src="images/profile.jpg"
												class="hoverZoomLink"></a>
									</div>
								</td>
							</tr>
						</tbody>
					</table>

					<table width="110%" align="center" border="0" cellspacing="0" cellpadding="20">
						<tbody>
							<tr>
								<td>
									<heading>News</heading>
								</td>
							</tr>
						</tbody>
					</table>
					<table width="110%" align="center" border="0" cellpadding="20">
						<tbody>
							<ul>
								<news>
									<li>[Feb. 2023] One paper about audio-visual saliency prediction is accepted to CVPR'23.</li>					
									<li>[Aug. 2022] One paper about multi-modal correlation learning is accepted by TMM'22.</li>
									<li>[June. 2022] Spent a wonderful summer
										interning at Horizon Robotics, working
										in <a
											href="https://www.horizon.ai/intelligent-cockpit.html">multimodal
											speech team</a>.
								</news>
							</ul>
						</tbody>
					</table>



					<table
						style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr>
								<td
									style="padding:20px;width:110%;vertical-align:middle">
									<heading>Projects</heading>
								</td>
							</tr>
						</tbody>
					</table>

					<table
						style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
								<td style="padding:20px;width:40%;vertical-align:left">
									<div><img style="width:100%;max-width:100%"
											src="images/FTFDNet.jpg"></div>
								</td>
								<td
									style="padding:20px;width:60%;vertical-align:middle">
									<papertitle>FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction
									</papertitle>
									<br>
									<br>
									<paper>Ganglai Wang, Peng Zhang, <b>Junwen Xiong</b>, Feihan Yang, Wei Huang, Yufei Zha
										<br>
									</paper>
									<papervenue> submitted to TCSVT, 2023</papervenue>
									<a
										href="https://arxiv.org/pdf/2307.03990.pdf">[paper]</a>
									<br>
									<paper>Incorporating three modalities to detect talking face video manipulation</paper>
								</td>
							</tr>
					</table>

					<table
						style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
								<td style="padding:20px;width:40%;vertical-align:left">
									<div><img style="width:100%;max-width:100%"
											src="images/CASP-Net.png"></div>
								</td>
								<td
									style="padding:20px;width:60%;vertical-align:middle">
									<papertitle>CASP-Net: Rethinking Video Saliency
										Prediction from an Audio-Visual
										Consistency Perceptual Perspective
									</papertitle>
									<br>
									<br>
									<paper>
										<b>Junwen Xiong</b>,
										Ganglai Wang, Peng Zhang, Wei Huang,
										Yufei Zha, Guangtao Zhai
										<br>
									</paper>
									<papervenue> CVPR, 2023</papervenue>
									<a
										href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_CASP-Net_Rethinking_Video_Saliency_Prediction_From_an_Audio-Visual_Consistency_Perceptual_CVPR_2023_paper.pdf">[paper]</a>
									<a href="CASP-Net/index.html">[webpage]</a>
									<br>
									<paper>Audio-visual consistency perception matters</paper>
								</td>
							</tr>
					</table>

					<table
						style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
								<td style="padding:20px;width:40%;vertical-align:left">
									<div><img style="width:100%;max-width:100%"
											src="images/ADENet.jpg"></div>
								</td>
								<td
									style="padding:20px;width:60%;vertical-align:middle">
									<papertitle>Look&Listen: Multi-Modal Correlation
										Learning for Active Speaker Detection
										and Speech Enhancement
									</papertitle>
									<br>
									<br>
									<paper>
										<b> Junwen Xiong</b>,
										Yu Zhou, Peng Zhang, Lei Xie, Wei Huang,
										Yufei Zha
										<br>
									</paper>
									<papervenue> TMM, 2022 </papervenue>
									<a
										href="https://arxiv.org/abs/2203.02216">[paper]</a>
									<a
										href="https://overcautious.github.io/ADENet/">[webpage]</a>
									<br>
									<paper>Unified correlation learning framework to solve two audio-visual tasks</paper>
								</td>
							</tr>
					</table>

					<table
						style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
								<td style="padding:20px;width:40%;vertical-align:left">
									<div><img style="width:100%;max-width:100%"
											src="images/av_speech.jpg">
									</div>
								</td>
								<td
									style="padding:20px;width:60%;vertical-align:middle">
									<papertitle>Audio-visual speech separation based
										on joint feature representation with
										cross-modal attention
									</papertitle>
									<br>
									<br>
									<paper>
										<b>Junwen Xiong</b>,
										Peng Zhang, Lei Xie, Wei Huang, Yufei
										Zha, Yanning Zhang
										<br>
									</paper>
									<papervenue> arXiv preprint, 2022</papervenue>
									<a
										href="https://arxiv.org/abs/2203.02655">[paper]</a>
									<br>
									<paper>Novel fusion methods for audio, video and optical flow modalities</paper>
								</td>
							</tr>
					</table>
