<!doctype html>
<html lang="en">


<!-- === Header Starts === -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Towards Unifying Saliency Transformer for Video Saliency Prediction and Detection</title>

	<link rel="icon" href="../images/letter-x.png" type="image/png">
  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


  <!-- === Home Section Starts === -->
  <div class="section">
    <!-- === Title Starts === -->
    <div class="header">
      <div class="title" , style="padding-top: 25pt;"> <!-- Set padding as 10 if title is with two lines. -->
        Towards Unifying Saliency Transformer for Video Saliency Prediction and Detection
      </div>
    </div>
    <!-- === Title Ends === -->
    <div class="author">
      <a href="https://scholar.google.com/citations?user=7iM62mUAAAAJ&hl=en" target="_blank">Junwen
        Xiong<sup>1</sup>,&nbsp;</a>
      <a href="#" target="_blank"> Chuanyue Li<sup>1</sup>,&nbsp;</a>
      <a href="#" target="_blank"> Tianyu Liu<sup>1</sup>,&nbsp;</a>
      <a href="https://teacher.nwpu.edu.cn/2011010119.html" target="_blank"> Peng Zhang<sup>1</sup>,</a>
      <a href="#" target="_blank"> Yue Huo<sup>1</sup>,&nbsp;</a>
      <a href="https://smcs.ncu.edu.cn/xxgk/xrld/e5556f612a974fcfafd7a53437528405.htm" target="_blank">Wei
        Huang<sup>2,3</sup>,</a>
      <a href="https://teacher.nwpu.edu.cn/yufeizha" target="_blank"> Yufei Zha<sup>1</sup>,</a>
    </div>
    <div class="institution">
      Northwestern Polytechnical University<sup>1</sup>
    </div>
    <div class="institution">
      Nanchang University<sup>2</sup>
    </div>
    <div class="institution">
      Yichun University<sup>3</sup>
    </div>
    <div class="conference">
      TCSVT, 2025
    </div>
   <div class="link" style="font-size: 14pt;">
      <a href="https://ieeexplore.ieee.org/document/10896756">[Paper]</a>&nbsp; <!--<a href="#">[Code]</a>&nbsp; <a href="#">[Data]</a> -->
    </div>
  </div>
  <!-- === Home Section Ends === -->


  <!-- === Overview Section Starts === -->
  <div class="section">
    <div class="title">Overview</div>
    <div class="body">
      Video saliency prediction and detection are thriving research domains that enable computers to simulate the distribution of visual attention akin to how humans perceive dynamic scenes.
While many approaches have crafted task-specific training paradigms for either video saliency prediction or video salient object detection tasks, few attention has been devoted to devising a generalized saliency modeling framework that seamlessly bridges both these distinct tasks.
In this study, we introduce the Unified Saliency Transformer (UniST) framework, which  comprehensively utilizes the essential attributes of video saliency prediction and video salient object detection. 
In addition to extracting representations of frame sequences, a saliency-aware transformer is designed to learn the spatio-temporal representations at progressively  increased resolutions, while incorporating effective cross-scale  saliency information to produce a robust representation.  
Furthermore, task-specific decoders are proposed to perform the final prediction for each task.  
To the best of our knowledge, this is the first work to explore the design of a unified framework for both saliency modeling tasks.
Convincible experiments demonstrate that the proposed UniST achieves superior performance across eight challenging  benchmarks for two tasks, outperforming other state-of-the-art methods in most metrics.

    </div>
  </div>
  <!-- === Overview Section Ends === -->


  <!-- === Overview Section Starts === -->
  <div class="section">
    <div class="title">Method</div>
    <!-- <div class="title">
    <img src="./assets/Figure1.png" width="800">
  </div> -->


    <div class="title">

      <div class="body">
        The VSP and VSOD adopt video encoder and image encoder respectively as feature extractors, followed by corresponding decoders. Differently, a unified saliency transformer directly applies an image encoder for feature processing, and follows a transformer structure for spatio-temporal modeling, and finally uses different decoders for different tasks.
      </div>

      <img src="./assets/img_1.png" width="800">

    </div>
    <div class="title">
      <div class="body">
        An overview of the proposed <font size="4" color="blue">UniST</font>. The visual feature encoder learns frame-wise visual representations from the video clips. 
        The multi-scale visual features are then used as inputs to the saliency-aware transformer for global spatio-temporal modeling, generating refined and scaled-up spatio-temporal features for final prediction. 
        The saliency transfer mechanism gathers the attention scores in each transformer stage and uses them to improve saliency prediction as well as salient object detection.
      </div>
      <img src="./assets/img_2.png" width="800">
    </div>
  </div>
  <!-- === Overview Section Ends === -->


  <!-- === Result Section Starts === -->
  <div class="section">
    <div class="title">Results</div>
    <div class="body">
      Qualitative results of our method compared with other state-of-the-art methods on VSP and VSOD tasks.
      <!-- Adjust the number of rows and columns (EVERY project differs). -->
      <table width="100%" style="margin: 20pt 0; text-align: center;">
        <tr>
          <td><img style="width: 100%;" src="./assets/img_3.png" height="90%", width="80%"></td>
        </tr>
        <tr>
          <td><img style="width: 100%;" src="./assets/img_4.png" height="90%"></td>
        </tr>
      </table>
      <div>
        Demo video.
      </div>
      <br>

      <table style="margin: 20pt 0; text-align: center;">
        <video width="100%" height="60%" controls>
          <source src="./assets/UniST_VSP_VSOD.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </table>
    </div>
  </div>
  <!-- === Result Section Ends === -->


  <!-- === Reference Section Starts === -->
  <!-- <div class="section">
    <div class="bibtex">BibTeX</div>
    <pre>
  @inproceedings{xiong2024diffsal,
    title={DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction},
    author={Junwen Xiong, Peng Zhang, Tao You, Chuanyue Li, Wei Huang and Yufei Zha},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2024}
  }
</pre> -->

    <!-- BZ: we should give other related work enough credits, -->
    <!--     so please include some most relevant work and leave some comment to summarize work and the difference. -->
    <!-- <div class="ref">Related Work</div>
  <div class="citation">
    <div class="image"><img src="./assets/tnt.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2008.08294.pdf" target="_blank">
        Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Congcong Li, Dragomir Anguelov.
        TNT: Target-driveN Trajectory Prediction.
        CoRL 2020.</a><br> -->
    <!-- <b>Comment:</b>
      This is a short comment. -->
    <!-- </div>
  </div>
  <div class="citation">
    <div class="image"><img src="./assets/vec2.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2005.04259.pdf" target="_blank">
        Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, Cordelia Schmid.
        VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation.
        CVPR 2020.</a><br> -->
    <!-- <b>Comment:</b>
      This is a long comment. This comment is just used to test how long comments can fit the template. -->
    <!-- </div>
  </div>
  <div class="citation">
    <div class="image"><img src="./assets/TP.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2004.12255.pdf" target="_blank">
        Liangji Fang, Qinhong Jiang, Jianping Shi, Bolei Zhou.
        TPNet: Trajectory Proposal Network for Motion Prediction.
        CVPR 2020.</a><br> -->
    <!-- <b>Comment:</b>
      This is a long comment. This comment is just used to test how long comments can fit the template. -->
    <!-- </div>
  </div>
</div> -->
    <!-- === Reference Section Ends === -->


</body>

</html>
